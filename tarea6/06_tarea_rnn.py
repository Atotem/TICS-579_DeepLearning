# -*- coding: utf-8 -*-
"""06-Tarea-RNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-bQhKJQdONTZbnX44_4J9VV9UVNei5K7

Integrantes:
Alberto Bella,
Fernando Contreras,
Ignacio Morandé
"""

import numpy as np
import tensorflow as tf
import pandas as pd
from plotnine import *
from sklearn import preprocessing
from tensorflow.keras import *
import matplotlib.pyplot as plt

"""# Predicción de COVID-19 (parte 1)
Como sabemos, el COVID-19 a causado estratagos a nivel mundial. El número de casos a aumentado en forma significativa y Chile no ha dejado de ser la excepción. Para esta tarea se le pide, utilizando redes neuronales recurrentes (básica, LSTM, o GRU), predecir el número de nuevos contagiados en Chile para los próximos 7 días después de la fecha de entrega oficial (entre el 9 de Junio y 15 de Junio).

La data de Chile y el mundo la podrá bajar en https://ourworldindata.org/coronavirus-source-data.

Evaluación:
1. Código y evaluación del modelo implementado (2 puntos)
2. Competencia basado en el MSE (1.5 puntos)
"""

file = pd.read_csv('owid-covid-data.csv')
file.describe()

file.head()

plt.figure(figsize=(10, 7))
plt.title('Confirmed Cases Chile 10-06-2021')
plt.xlabel('date')
plt.ylabel('new cases')
plt.plot(file.loc[file['location']=="Chile", ['new_cases_smoothed']])
plt.show()

datos = file.loc[file['location']=="Chile", ['new_cases_smoothed']]
datos = np.array(datos)
print(datos.shape)
datos = np.nan_to_num(datos)
datos = pd.DataFrame(datos)
print(type(datos))
print("Datos: ", datos)

Tp = 400
train, test = datos[0:Tp], datos[Tp:len(datos)]
timeSteps = 5
numDim = 1
train = np.array(train)
test = np.array(test)
print(type(train))

"""
[1, ->  [[1,2,3],[4]],
 2,     [[2,3,4],[5]],
 3,     [[3,4,5],[6]],
 4,     [[4,5,6],[7]],
 5,     [[5,6,7],[8]],
 6,     [[6,7,8],[9]],
 7,
 8,
 9]
"""

def convertToMatrix(data, timeSteps):
    X, Y =[], []
    for i in range(len(data)-timeSteps):
        d=i+timeSteps  
        X.append(data[i:d,])
        Y.append(data[d,])
    return np.array(X), np.array(Y)

trainX, trainY = convertToMatrix(train, timeSteps)
testX, testY = convertToMatrix(test, timeSteps)
#print("TrainX: ", trainX)
#print("TrainY: ", trainY)

trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1],1))
testX = np.reshape(testX, (testX.shape[0],testX.shape[1],1))

inputLayer = layers.Input(shape=(timeSteps, numDim))
hiddenLayer = layers.SimpleRNN(units=20, activation="tanh", use_bias=True,return_sequences=False,return_state=False)(inputLayer)
outputLayer = layers.Dense(units=1,activation="linear")(hiddenLayer)
RNN = models.Model(inputLayer, outputLayer)
print(RNN.summary())

RNN.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['MeanSquaredError'])
history = RNN.fit(trainX,trainY, epochs=100, batch_size=16, validation_data=(testX, testY), verbose=0)
print(history.history['mean_squared_error'])
trainPredict = RNN.predict(trainX)
testPredict= RNN.predict(testX)

plt.figure(figsize=(10, 7))
plt.title('Confirmed Cases Chile 10-06-2021 Predicted')
plt.xlabel('date')
plt.ylabel('new cases')
plt.plot(testPredict)
plt.show()

trainScore = RNN.evaluate(trainX, trainY, verbose=0)
testScore = RNN.evaluate(testX, testY, verbose=0)
print(trainScore,testScore)

"""# Creando tweets (parte 2)
Una de las ventajas de las redes recurrentes es su uso con texto. Una de ellas es la generación de frases, en este caso, tweets. Para la segunda parte de la tarea se le pide:
1. Extraer 100/200 tweets de alguna cuenta en twitter (a su elección). Obviamente mientras más tweets es mejor, pero lamentablemente con muchos tweets tomará mucho tiempo entrenar el modelo (1 punto). En caso que no logre extraer los datos, solicite al profesor una base de tweets (pero obtendrá 0 puntos en este caso)
2. Aprender una red neuronal recurrente (0.5 puntos).
3. Generar cuatro frases que comiencen con las palabras "La", "El", "Hoy", y alguna palabra a su elección (1 punto), las frases deberán tener entre 5 y 15 palabras (sampleadas de forma uniforme). En caso que alguna palabra base no exista en su vocablo, entonces reemplácela por alguna otra palabra común. Además, en vez de considerar la palabra con mayor probabilidad para generar la frase, considere una palabra sampleada desde las 10 palabras con mayor probabilidad, con probabilidad proporcional a su valor (recuerde estandarizar las probabilidades para que no sea una distribución uniforme). De esta forma no generará siempre la misma palabra.

En su entrega final, usted debera entregar el código y la base de datos correspondiente. 

para solucionar esta tarea recuerde la actividad, usted deberá:
1. Tokenizar los datos
2. Crear las secuencias las frases. Por ejemplo de la frase "Hola como estas tu?" se generan tres frases, "Hola como", "Hola como estas", y "Hola como estas tu?". Las secuencias son al menos dos palabras, ya que la última palabra de cada secuencia será la palabra a predecir.
3. Hacer padding de todas las secuencias creadas anteriormente (todas las frases tienen tamaño d)
4. Separar los datos de entrenamiento en los primeros d-1 "palabras" como entrenamiento y la última palabra como objetivo.
5. Entrenar el modelo (recuerde que la variable Y tiene que ser traspasada a one-hot-encoding)
6. Predecir una frase. Para ello, parte con una palabra inicial de su elección, prediga la segunda palabra, agreguela a su input y prediga la siguiente. Repita este rpoceso hasta que el número de palabras que ustede definió ya fueron creadas.

Por si acaso, es sumamente factible que la frase generada no haga sentido común. No se descontará puntos por este tipo de casos. Pero el código deberá correr sin problemas.
"""

import pandas as pd
import numpy as np
from tensorflow.keras import *
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
import matplotlib.pyplot as plt

data = pd.read_csv('ingenieriaUAI_tweets.csv', nrows=200)
data.head()

#Tokenizacion, determinacion tamaño diccionario y creacion secuencias
text = data['text'].tolist()
tokenizer = Tokenizer()
tokenizer.fit_on_texts(text) #Train tokenizer

vocab_size = len(tokenizer.word_index) + 1

encoded = tokenizer.texts_to_sequences(text) #Encoding

print(text)
print(encoded)

#Padding

max_len = 40
data = pad_sequences(encoded, maxlen = max_len)
print(data)

# Entrenamiento y palabra objetivo con one-hot encoding

X = data[:max_len]
y = data[-1]

y = to_categorical(y, num_classes=vocab_size)

#Modelo

inputLayer = layers.Input(shape=(1,))
embeddingLayer = layers.Embedding(vocab_size, 10, input_length=1)(inputLayer)
RNNLayer = layers.SimpleRNN(units=50)(embeddingLayer)
outputLayer = layers.Dense(units=vocab_size, activation='softmax')(RNNLayer)

model = models.Model(inputLayer, outputLayer)
print(model.summary())
origWeights=model.get_weights()[2]

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X, y, epochs=500, verbose=0)

#Checking the change of parameters of the W_{hh} matrix
finalWeights=model.get_weights()[2]
print(sum(sum(origWeights==finalWeights)))

def generate_seq(model, tokenizer, seed_text, n_words):
  in_text, result = seed_text, seed_text
  # generate a fixed number of words
  for _ in range(n_words):
    # encode the text as integer
    encoded = tokenizer.texts_to_sequences([in_text])[0]
    encoded = np.array(encoded) # encode the text as integer
    
    # predict probabilities for each word
    probs = model.predict(encoded, verbose=0) # predict probs per word
    norm = np.linalg.norm(probs)
    probs = probs/norm
    yhat = np.where(probs[0] == np.amax(probs[0]))
    yhat = yhat[0] # Obtaining the word with highest probability
    
    #Cambiar probabilidades

    # map predicted word index to word
    out_word = '' # Getting the word from the tokenizer
    for word, index in tokenizer.word_index.items():
      if index == yhat:
        out_word = word
        break
    # Appending to the output and getting the new input
    in_text, result = out_word, result + ' ' + out_word
  return result

# Evaluating the model
print(generate_seq(model, tokenizer, 'La', 15))
print(generate_seq(model, tokenizer, 'El', 15))
print(generate_seq(model, tokenizer, 'Hoy', 15))
print(generate_seq(model, tokenizer, 'Este', 15))