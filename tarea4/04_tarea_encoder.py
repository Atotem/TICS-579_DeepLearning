# -*- coding: utf-8 -*-
"""04-Tarea-Encoder.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15ij2cDOVHXEyXmGS0mwKPHA4DYlp-cdv

## Integrantes:
1. Ignacio Morande
2. Fernando Contreras
3. Alberto Bella

# Tarea 4

El archivo adultTrain.csv tiene tiene la información de 26048 personas incluídas el tipo de trabajo, genero, país de procedencia, entre otras variables. Para esta tarea se le pide clasificar si una persona gana más de 50K (última variable). Específicamente:
1. Cree una red neuronal como usted estime conveniente y estime el error del modelo entrenado (1 punto).
2. Entrene una red encoder/decoder sobre las variables nominales (1.5 puntos).
3. Cree una nueva red neuronal utilizando los datos origianles que estime conveniente y generados por el encoder del punto 2 (1.5 punto). Obviamente, no incluya una misma variable dos veces. Es decir, si incluyó una variable nominal en este punto, no incluya la misma variable codificada.
4. Compare los errores de los modelos y seleccione el mejor (0.5 puntos).
5. Clasifique los datos de adultEval.csv y entregue un archivo csv con la clasificación de cada punto (1.5 puntos). El archivo tendrá una sola columna indicando <=50K o >50K para cada uno de los puntos de evaluación. Tal como en la tarea 2, su puntaje dependerá de su rendimiento comparado con el resto. Especificamente, se evaluará el F1-score donde positive corresponderá a la clase más pequeña.

Justifique todo el proceso realizado, por ejemplo creación de variables, selección de modelos, verificación de overfitting, y/u otros.

## Descripción de variables
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

data_train = pd.read_csv('adultTrain.csv')
data_eval = pd.read_csv('adultEval.csv')

data_train.head()

data_eval.head()

data_train.describe()

data_train.corr()

fig, ax = plt.subplots(3, 3, figsize=(15, 15))

ax[0,0].set_title('workclass')
ax[0,0].hist(data_train['workclass'])

ax[0,1].set_title('education')
ax[0,1].hist(data_train['education'])

ax[0,2].set_title('marital.status')
ax[0,2].hist(data_train['marital.status'])

ax[1,0].set_title('occupation')
ax[1,0].hist(data_train['occupation'])

ax[1,1].set_title('relationship')
ax[1,1].hist(data_train['relationship'])

ax[1,2].set_title('race')
ax[1,2].hist(data_train['race'])

ax[2,0].set_title('sex')
ax[2,0].hist(data_train['sex'])

ax[2,1].set_title('native.country')
ax[2,1].hist(data_train['native.country'])

ax[2,2].set_title('income')
ax[2,2].hist(data_train['income'])

data_train.isna().sum()

"""## Preprocesamiento"""

# from sklearn.preprocessing import QuantileTransformer
from sklearn.preprocessing import StandardScaler

ndata = ['age', 'fnlwgt', 'education.num', 'capital.gain', 'capital.loss', 'hours.per.week']

scaler = StandardScaler()
scaler.fit(data_train[ndata])
data_train[ndata] = scaler.transform(data_train[ndata])
# data_train[ndata]

data_train['income'] = data_train['income'].replace({'>50K':0, '<=50K':1})

from tensorflow.keras.utils import to_categorical

data_train_onehot = pd.DataFrame()
cdata = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'native.country']

for col in cdata:
  data_train[col] = pd.Categorical(data_train[col])
  data_train[col] = data_train[col].cat.codes

  temp = pd.DataFrame(to_categorical(data_train[col]).astype('int'))
  temp.columns = [str(col) + str(tempCol) for tempCol in temp.columns]
  data_train_onehot = pd.concat([data_train_onehot, temp], axis=1)

data_train = data_train.drop(columns=cdata)
total_data_train = pd.concat([data_train, data_train_onehot], axis=1)

total_data_train

"""## Parte 1: Modelo FFN"""

# from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split
from tensorflow.keras import *
from keras import callbacks

earlystopping = callbacks.EarlyStopping(monitor='val_binary_accuracy', patience=50, restore_best_weights=True, verbose=0)

X = total_data_train.drop(columns='income')
y = total_data_train['income']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=42)

inputLayer = layers.Input(shape=(108,))
hiddenLayer = layers.Dense(108, activation='relu', use_bias=True, kernel_regularizer=regularizers.l2(0.005))(inputLayer)
hiddenLayer1 = layers.Dense(108, activation='relu', use_bias=True, kernel_regularizer=regularizers.l2(0.005))(hiddenLayer)
hiddenLayer2 = layers.Dense(108, activation='relu', use_bias=True, kernel_regularizer=regularizers.l2(0.005))(hiddenLayer1)
outputLayer = layers.Dense(1, activation='sigmoid', use_bias=True)(hiddenLayer2)

feedForward = models.Model(inputLayer, outputLayer)
feedForward.compile(loss ='BinaryCrossentropy', optimizer = 'adam', metrics = ['binary_accuracy'])
eval = feedForward.fit(X_train[:1000], y_train[:1000], validation_data = (X_test, y_test), batch_size = 32, epochs = 100, verbose = 0, shuffle = True, callbacks=[earlystopping])

plt.figure(figsize=(10, 8))
plt.title('Loss vs Epoch')
plt.plot(eval.history['loss'], label='training')
plt.plot(eval.history['val_loss'], label='validation')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend()
plt.show()

plt.figure(figsize=(10, 8))
plt.title('Acc vs Epoch')
plt.plot(eval.history['binary_accuracy'], label='training')
plt.plot(eval.history['val_binary_accuracy'], label='validation')
plt.ylabel('Acc')
plt.xlabel('Epoch')
plt.legend()
plt.show()

data_predicted = feedForward.predict(X_test)
data_predicted = (data_predicted > 0.2).astype(int)

plt.hist(data_predicted)
plt.show()

from sklearn.metrics import f1_score

f1_score(data_predicted, y_test)

"""## Parte 2: Autoencoder"""

# SINGLE AUTOENCODER
import pandas as pd
from tensorflow.keras import *

nominal_data = data_train_onehot
#nominal_data = pd.read_csv('adultTrain.csv')
#nominal_data = nominal_data.iloc[:,[1,3,5,6,7,8,9,13]]

col_num = 102
encoding_num = 20

# Creación red
input_layer = layers.Input(shape=(col_num,))
hidden_layer = layers.Dense(encoding_num, activation='linear',use_bias = True)(input_layer)
output_layer = layers.Dense(col_num, activation='linear',use_bias = True)(hidden_layer)

# Creación modelo
autoencoder = models.Model(input_layer, output_layer)
autoencoder.compile(loss='mse', optimizer = 'adam')
eval = autoencoder.fit(nominal_data, nominal_data,epochs=100,batch_size=32,shuffle=True,verbose = 0)

#Errores
#MSE = []
#MSE.append(np.mean(eval.history['mse']))
#MSE
print(eval.history)

# Selección desde la capa de entrada y el espacio latente, y encoding de los datos
encoder = models.Model(input_layer, hidden_layer)
encoded_data = encoder.predict(nominal_data)

df_nominal = pd.DataFrame(encoded_data, columns=['nominal '+ str(i) for i in range(encoding_num)])

# Creación red para decoder
decoder_input = layers.Input(shape=(encoding_num,))
decoder_layer = autoencoder.layers[-1] # Última capa del modelo autoencoder

# Creación modelo del decoder y decoding de los datos
decoder = models.Model(decoder_input, decoder_layer(decoder_input))
decoded_data = decoder.predict(encoded_data)

#Parte 3

data_parte3 = pd.concat([data_train, df_nominal], axis=1)
data_parte3

"""## Parte 3: Modelo + Datos codificados"""

# from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split
from tensorflow.keras import *
from keras import callbacks

earlystopping = callbacks.EarlyStopping(monitor='val_binary_accuracy', patience=50, restore_best_weights=True, verbose=0)

X = data_parte3.drop(columns='income')
y = data_parte3['income']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=42)

inputLayer = layers.Input(shape=(26,))
hiddenLayer = layers.Dense(26, activation='relu', use_bias=True, kernel_regularizer=regularizers.l2(0.005))(inputLayer)
hiddenLayer1 = layers.Dense(26, activation='relu', use_bias=True, kernel_regularizer=regularizers.l2(0.005))(hiddenLayer)
hiddenLayer2 = layers.Dense(26, activation='relu', use_bias=True, kernel_regularizer=regularizers.l2(0.005))(hiddenLayer1)
outputLayer = layers.Dense(1, activation='sigmoid', use_bias=True)(hiddenLayer2)

feedForward = models.Model(inputLayer, outputLayer)
feedForward.compile(loss ='BinaryCrossentropy', optimizer = 'adam', metrics = ['binary_accuracy'])
eval = feedForward.fit(X_train[:1000], y_train[:1000], validation_data = (X_test, y_test), batch_size = 32, epochs = 100, verbose = 0, shuffle = True, callbacks=[earlystopping])

plt.figure(figsize=(10, 8))
plt.title('Loss vs Epoch')
plt.plot(eval.history['loss'], label='training')
plt.plot(eval.history['val_loss'], label='validation')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend()
plt.show()

plt.figure(figsize=(10, 8))
plt.title('Acc vs Epoch')
plt.plot(eval.history['binary_accuracy'], label='training')
plt.plot(eval.history['val_binary_accuracy'], label='validation')
plt.ylabel('Acc')
plt.xlabel('Epoch')
plt.legend()
plt.show()

data_predicted = feedForward.predict(X_test)
data_predicted = (data_predicted > 0.2).astype(int)

plt.hist(data_predicted)
plt.show()

from sklearn.metrics import f1_score

f1_score(data_predicted, y_test)

"""## Parte 4: Comparación de modelos

1. Modelo FFN
- F1_score: 0.885

2. Modelo FFN + encoded data
- F1_score: 0.887

## Parte 5: Predicción de variables
"""

# data_eval
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.utils import to_categorical

ndata = ['age', 'fnlwgt', 'education.num', 'capital.gain', 'capital.loss', 'hours.per.week']

scaler = StandardScaler()
scaler.fit(data_eval[ndata])
data_eval[ndata] = scaler.transform(data_eval[ndata])

data_eval_onehot = pd.DataFrame()
cdata = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'native.country']

for col in cdata:
  data_eval[col] = pd.Categorical(data_eval[col])
  data_eval[col] = data_eval[col].cat.codes

  temp = pd.DataFrame(to_categorical(data_eval[col]).astype('int'))
  temp.columns = [str(col) + str(tempCol) for tempCol in temp.columns]
  data_eval_onehot = pd.concat([data_eval_onehot, temp], axis=1)

data_eval = data_eval.drop(columns=cdata)
total_data_eval = pd.concat([data_eval, data_eval_onehot], axis=1)

total_data_eval

input_layer = layers.Input(shape=(100,))
hidden_layer = layers.Dense(20, activation='linear',use_bias = True)(input_layer)
output_layer = layers.Dense(100, activation='linear',use_bias = True)(hidden_layer)

autoencoder = models.Model(input_layer, output_layer)
autoencoder.compile(loss='mse', optimizer = 'adam')
eval = autoencoder.fit(data_eval_onehot, data_eval_onehot, epochs=100,batch_size=32,shuffle=True,verbose = 0)

encoder = models.Model(input_layer, hidden_layer)
encoded_data = encoder.predict(data_eval_onehot)

data_parte5 = pd.concat([data_eval, pd.DataFrame(encoded_data)], axis=1)
data_parte5

data_eval_predicted = feedForward.predict(data_parte5)
data_eval_predicted = (data_eval_predicted > 0.2).astype(int)
data_eval_predicted

plt.hist(data_eval_predicted)
plt.show()

data_eval_predicted = pd.DataFrame(data_eval_predicted)

data_eval_predicted = data_eval_predicted.replace({0:'>50K', 1:'<=50K'})
data_eval_predicted = data_eval_predicted.rename(columns={0: 'income'}, inplace = False)

data_eval_predicted.to_csv('data_eval_predicted.csv', index=False)
# data_eval_predicted